ETL Pipelines: Design & Implementation:


1. Overview

	The ETL (Extract, Transform, Load) pipeline is responsible for processing data from the on-prem SQL Server database to Azure Synapse Analytics, passing through 	Bronze, Silver, and Gold layers in Azure Data Lake Storage Gen2 (ADLS Gen2).

	Key Features of the ETL Pipeline
		✅ Dynamic Pipeline: Built to handle multiple tables dynamically.
		✅ Incremental Loading: Ensures only new or updated data is processed.
		✅ Medallion Architecture: Bronze (Raw), Silver (Cleaned), and Gold (Aggregated).
		✅ Scalable & Automated: Uses Azure Data Factory and Databricks workflows.

2. Dynamic ETL Pipeline

	2.1 Why Dynamic ETL?
		Instead of creating separate pipelines for each table, a single dynamic pipeline processes all tables.

		Uses metadata-driven design with Lookup Activity and ForEach Loop in ADF.


	2.2 ETL Pipeline Architecture
		The ETL process follows three stages:

			Stage 1: Data Extraction (Bronze Layer)
				Source: On-Prem SQL Server
				Destination: ADLS Gen2 (Bronze Layer - Parquet Format)
				Tools: Azure Data Factory (ADF)

				✅ Pipeline Name: SQL_to_ADLS_Bronze
				✅ Activities Used: Lookup → ForEach → Copy Data

			Steps
				Metadata-Driven Lookup

				A Lookup Activity retrieves the list of all tables from SQL Server.

		
			Loop Through Tables:

				ForEach Activity loops through the table list.

				Inside the loop:

					Copy Data Activity moves data from SQL Server to ADLS Gen2 (Bronze Layer) in Parquet format.

					Dynamic Parameters in ADF


			✅ Outcome: All tables are dynamically ingested into the Bronze Layer.

			
			Stage 2: Data Transformation (Silver Layer)
				
				Source: ADLS Gen2 (Bronze)
				Destination: ADLS Gen2 (Silver - Delta Format)
				Tools: Azure Databricks (PySpark)

				✅ Pipeline Name: Bronze_to_Silver
				✅ Activities Used: Databricks Notebook Activity


			Steps

				Databricks Notebook Execution

				Reads data from Bronze Layer.

				Cleans and transforms data (renaming columns, handling nulls, formatting dates).

				Saves data in Delta format in Silver Layer.

			
			✅ Outcome: Data is cleaned and transformed, ready for dimensional modeling.


			Stage 3: Data Modeling (Gold Layer)

				Source: ADLS Gen2 (Silver)
				Destination: ADLS Gen2 (Gold - Delta Format)
				Tools: Azure Databricks (PySpark)

				✅ Pipeline Name: Silver_to_Gold
				✅ Activities Used: Databricks Notebook Activity


			Steps

				Transform & Join Silver Layer Tables

				DimCustomer: Joins Customer, CustomerAddress, Address

				DimProduct: Joins Product, ProductCategory, ProductModel

				FactSales: Joins SalesOrderHeader, SalesOrderDetail

				FactSalesAggregation: Performs aggregations on FactSales


			✅ Outcome: Gold layer is populated with optimized, analytics-ready data.




3. Final Execution Flow

		1️ ADF Pipeline Runs (SQL → ADLS Bronze)
		2️ Databricks Notebook Runs (Bronze → Silver)
		3️ Databricks Notebook Runs (Silver → Gold)
		4️ Gold Layer Data is Queried in Synapse Analytics
		5️ Power BI Fetches Data for Reporting

5. Summary

		✅ Dynamic & Scalable – Handles multiple tables.
		✅ Incremental Loading – Loads only new data.
		✅ Optimized for Analytics – Gold layer supports BI reporting.

